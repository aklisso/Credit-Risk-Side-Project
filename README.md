# Prediction of credit risk using supervised machine learning algorithms and stacked classifiers in R

## Overview
This project aims to predict credit risk (good vs poor) using supervised machine learning algorithms in R. Six base models were evaluated against two stacked classifiers, one which was trained “manually” on predictions generated by the base models, and one which was trained automatically using the caretStack method in R. The models were built and evaluated using the German Credit Risk dataset, which was accessed through Kaggle. In this dataset, each entry represents an individual who takes credit by a bank.

## Data Preparation
### Data cleaning and partitioning
To clean the data, variable names were translated to English, and sparse categories were collapsed to avoid model instability. The univariate distributions of continuous variables were assessed for skewness/violations of normality, which were present. This would impact later modeling. After splitting the data into training (70%), validation (20%), and test (10%) datasets, GVIF values were examined in the training dataset to rule out multicollinearity.
### Filter-based feature selection
Although multicollinearity was not present, filter-based feature selection was performed using information gain criteria, which measures the reduction in uncertainty of the target variable when a feature of interest is known. This was to replicate the results of a [journal article](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00882-0) , as well as gain familiarity with a new method of reducing multicollinearity if it arises in future work. Feature selection can also speed up model training by reducing the volume of data. 

The top thirteen features were selected (as in the referenced article), and the following were eliminated:
-	Installment rate
-	Job
-	Present residence
-	People liable
-	Age
-	Telephone
-	Number of credits

## Model Training
### Method 1: Manual model training/stacked classifier
First, six ML models were trained separately and hyperparameters were tuned by inspection and/or conducting an automated search to maximize the AUC on validation data. Models came from various R packages, which were selected based on online documentation. 
The following models were used:
-	Random forest (RF)
-	Gradient boosting (GB)
-	K nearest neighbors (KNN)
-	Artificial neural network (ANN)
-	Elastic net logistic regression
-	Extreme gradient boosting (XGB)
  
Data were further processed independently for each model to ensure statistical assumptions were met and data were formatted properly to be used as model inputs (i.e. conversion to dummy variables, log transformations/standardization, etc.). 
Predictions were generated on validation data, which were used to train a meta-model (logistic regression). Then, the base models were used to generate predictions on the test data, which were used as inputs to evaluate the meta-model’s performance.
Base models were evaluated against the stacked classifier based on accuracy, AUC, and F1 score (table 1).
### Method 2: Stacked classifier using caretStack
To determine whether a comparable stacked classifier could be built more efficiently, the caret package was used to automate the process of training/testing the stacked classifier. The validation and test datasets previously created were aggregated into a singular test dataset, since caret trains models using cross-validation, hence there was no need for a validation set. First, continuous variables in the training data were log-transformed to address skewness, and standardized to ensure compatibility with caret’s base models. These transformations were applied to the test data. The base models were then automatically trained/tuned using caretList with cross-validation to attain hyperparameters that maximized the AUC.
Similarity between base models was evaluated using the modelCor function – if the similarity between two models exceeded 0.8, the model with the lower AUC on the training data was removed to reduce redundancy in the stacked classifier.
A stacked classifier was trained/tested using caretStack, and evaluated based on accuracy, AUC, and F1 score (Table 1).
Results and comparison of models
Table 1 displays the candidate models’ accuracy, AUC, and F1 scores.
| Model	| Accuracy	| AUC	| F1 Score |
|---|---:|---:|---:|
| Random Forest	| 0.8085	| 0.8137	| 0.7076923	| 
| Gradient Boosting	| 0.7745	| 0.8305	| 0.6760563	| 
| XGBoost	| 0.7647	| 0.7946	| 0.6666667	| 
| Stacked Classifier (“manual”)	| 0.8039 | 0.7815 | 0.6666667 |
| Elastic net | 0.6961	| 0.8096	| 0.6436782 |
| KNN	| 0.7255	| 0.7715	| 0.6216216 |
| Artificial Neural Network	| 0.7647	| 0.7438	| 0.6129032 |
| Stacked Classifier (caretStack)	| 0.7152	| 0.7769	| 0.5943396 |

*Table 1: Accuracy, AUC, and F1 score of candidate models*

Surprisingly, neither stacked classifier outperformed every base model, which departs from the results of the article cited above. Out of all the models, Random Forest performed the best (Table 1), with the highest accuracy and F1 score and the second-highest AUC attained (only 2% lower than the model with the highest AUC). The “manually-created” stacked classifier outperformed the caretStack stacked classifier with respect to all metrics examined, likely due to more extensive tuning of base models.

## Implementation
The data used is contained in the *german_credit_data.csv* file. To prepare the data, that file is read into *cleaning_eda.R*, which produces cleaned train, validation, and test datasets (called *credit_train.csv*, *credit_valid.csv*, and *credit_test.csv*, respectively. These files are then processed in *data_processing.R*, during which feature selection takes place, producing *train_fs_ig.csv*, *valid_fs_ig.csv*, and *test_fs_ig.csv*. Each of the base models is trained in a separate R file (titled *\<model_name\>.R*), each generating a CSV file of predicted values on the validation/testing data (titled *\<model_name\>_pred_valid.csv* and *\<model_name\>_pred_test.csv* respectively). These are used as inputs for the meta model in *stacked_classifier.R*. The "automated" version of the classifier is entirely trained/tested in *stacked_classifier_caret.R*
